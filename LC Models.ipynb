{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fC9T_YvLqXRc",
        "U2TEWF4eqXSY",
        "C9rqKXmSqXTS",
        "aBizde3qqXUA",
        "rjFi8irfqXU-",
        "MLdCoVtXqXVW",
        "HQxDe4itqXVo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tleitch/BDML/blob/main/LC%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F568INsBO9Ny"
      },
      "source": [
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_391lavQ_5l"
      },
      "source": [
        "**Install below package using terminal**\n",
        "\n",
        "\"conda install -c glemaitre imbalanced-learn\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVHDHiaCtVcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6eff5f-bf02-4e99-df8d-5fd5d78ea76d"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -U sklearn\n",
        "!pip install imblearn\n",
        "!pip install textblob\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install xgboost\n",
        "!pip install -U seaborn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 11.3MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 17.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 21.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 15.5MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 14.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 15.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 11.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 11.4MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 10.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 10.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122kB 10.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133kB 10.9MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 10.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153kB 10.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163kB 10.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174kB 10.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184kB 10.9MB/s eta 0:00:01\r\u001b[K     |████                            | 194kB 10.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 10.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 10.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 245kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 266kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 276kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 327kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 337kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 348kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 358kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 368kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 378kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 389kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 399kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 409kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 419kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 440kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 450kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 460kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 471kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 491kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 501kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 512kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 522kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 532kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 542kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 552kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 563kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 573kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 583kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 593kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 604kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 614kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 634kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 645kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 655kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 665kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 675kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 686kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 696kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 706kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 716kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 727kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 737kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 747kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 757kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 768kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 778kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 788kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 798kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 808kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 819kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 829kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 839kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 849kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 860kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 870kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 880kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 890kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 901kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 911kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 921kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 931kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 942kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 952kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 962kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 972kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 983kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 993kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.0MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.0MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.0MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 10.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0.1\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (1.0.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Collecting scikit-learn>=0.24\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: threadpoolctl, scikit-learn, imbalanced-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.4.3\n",
            "    Uninstalling imbalanced-learn-0.4.3:\n",
            "      Successfully uninstalled imbalanced-learn-0.4.3\n",
            "Successfully installed imbalanced-learn-0.8.0 scikit-learn-0.24.1 threadpoolctl-2.1.0\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6kC__i_O9N-",
        "outputId": "1a99e4f3-d514-4fa5-bf7c-a9d56053dec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.api import OLS\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import math\n",
        "from scipy.special import gamma\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQXD8AnuqW1m"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import resample\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.svm import SVC \n",
        "from xgboost.sklearn import XGBClassifier \n",
        "import itertools\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "from sklearn.ensemble import VotingClassifier \n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LksceGgR7DKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1fcac4-97f0-4fa8-ee00-58935f723a80"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iLFeAhNqW2U"
      },
      "source": [
        "# Prediction of Charge-Offs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0rSGn2qqW2g"
      },
      "source": [
        "We will consider the loan status as the response variable, a binary outcome for a loan with value 0 for fully paid and 1 for Charged Off.\n",
        "\n",
        "We will work with data previously cleaned and augmented with census information. We will use a subset of loans which were fully paid or charged-off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV4nF3QB7DKm"
      },
      "source": [
        "np.random.seed(9999)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksahczBTqW2s"
      },
      "source": [
        "# Entire data - please use this data set when you run it on AWS or a powerful machine.\n",
        "#df_loan_accepted_census_cleaned = pd.read_csv('http://digintu.tech/tmp/cs109a/df_loan_accepted_census_cleaned_closed_2007-2015.csv')\n",
        "\n",
        "# 1% of the data\n",
        "# 1% of all closed loans between 2007 and 2015, cleaned and augmented with census data - 73 MB\n",
        "#df_loan_accepted_census_cleaned = pd.read_csv('http://digintu.tech/tmp/cs109a/df_loan_accepted_census_cleaned_closed_2007-2015_10.csv').sample(frac=.1) \n",
        "\n",
        "# 10% of the data\n",
        "# 10% of all closed loans between 2007 and 2015, cleaned and augmented with census data - 73 MB\n",
        "df_loan_accepted_census_cleaned = pd.read_csv('https://s3.amazonaws.com/ruxton.ai/df_loan_accepted_census_cleaned.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwRw5erdqW3w"
      },
      "source": [
        "df_loan = df_loan_accepted_census_cleaned.copy()\n",
        "df_loan = df_loan[df_loan.loan_status.isin(['Charged Off', 'Fully Paid'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhU8sq1duN-G"
      },
      "source": [
        "## Features Selection\n",
        "\n",
        "Our goal is now to do exploratory analysis using predictive models in order to find important features in closed loans.\n",
        "\n",
        "Statistical tests can be used to select features that have the strongest relationship with the response variable. \n",
        "\n",
        "The Recursive Feature Elimination works by recursively removing variables and building a model on those variables that remain. Model accuracy is used to select the variables which contribute the most to the response.\n",
        "\n",
        "In this section we use a model-based approach of features selection using bagged trees and PCA. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-i_hNY4qW4W"
      },
      "source": [
        "### Manual features selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJY4iWKKqW4m"
      },
      "source": [
        "Using domain knowledge, we remove a list of features as manual feature selection. \n",
        "The following list of predictors are those which we MUST not use since they are data gathered AFTER the loan is funded. The reason to exclude them is because these features will not be available in unseen future dataset. Those features are highly correlated to charged-off loans and would otherwise bias our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSX3NmPeqW42"
      },
      "source": [
        "not_predictor = [\n",
        "'chargeoff_within_12_mths',   \n",
        "'collection_recovery_fee',\n",
        "'debt_settlement_flag',\n",
        "'debt_settlement_flag_date',\n",
        "'deferral_term',\n",
        "'funded_amnt_inv',\n",
        "'funded_amnt',\n",
        "'hardship_amount',\n",
        "'hardship_dpd',\n",
        "'hardship_end_date',\n",
        "'hardship_flag',\n",
        "'hardship_last_payment_amount',\n",
        "'hardship_length',\n",
        "'hardship_loan_status',\n",
        "'hardship_payoff_balance_amount',\n",
        "'hardship_reason',\n",
        "'hardship_start_date',\n",
        "'hardship_status',\n",
        "'hardship_type',\n",
        "'last_credit_pull_d',\n",
        "'last_fico_range_high',\n",
        "'last_fico_range_low',\n",
        "'last_pymnt_amnt',\n",
        "'last_pymnt_d',\n",
        "'next_pymnt_d',\n",
        "'orig_projected_additional_accrued_interest',\n",
        "'out_prncp',\n",
        "'out_prncp_inv',\n",
        "'payment_plan_start_date',\n",
        "'pymnt_plan',\n",
        "'recoveries',\n",
        "'settlement_amount',\n",
        "'settlement_date',\n",
        "'settlement_percentage',\n",
        "'settlement_status',\n",
        "'settlement_term',\n",
        "'total_pymnt',\n",
        "'total_pymnt_inv',\n",
        "'total_rec_int',\n",
        "'total_rec_late_fee',\n",
        "'total_rec_prncp',\n",
        "'verification_status'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECKdkAx8qW5c"
      },
      "source": [
        "**Dropping non-relevant columns**\n",
        "\n",
        "\n",
        "Features like 'index', 'id', 'url' and so on are dropped as they are not relevant to our goal of loan status prediction.\n",
        "\n",
        "We drop the index column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCNLT3ALqW52"
      },
      "source": [
        "not_predictor  += ['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ert_v5B_qW6Q"
      },
      "source": [
        "We drop the id, URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCBCk-N2qW6a"
      },
      "source": [
        "not_predictor  += ['id', 'url']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNa6R9J0qW62"
      },
      "source": [
        "We drop the employment title, loan title and description, which contains too many distinct values and cannot be easily categorized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu-6NJWeqW7A"
      },
      "source": [
        "not_predictor += ['emp_title', 'title', 'desc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojmvxMuMqW7m"
      },
      "source": [
        "We drop the success flag since it contains the same information as the loan status."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnQzWPC8qW72"
      },
      "source": [
        "not_predictor += ['success']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV-YYNbpqW8Y"
      },
      "source": [
        "We remove following the issue date, quarter and year, which is less relevant for future loans, we only keep the issue month (Jan to Dec)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T37i6WncqW8i"
      },
      "source": [
        "not_predictor += ['issue_q', 'issue_d']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mzlFLdVqW82"
      },
      "source": [
        "The grade and subgrade are categories which the LendingClub match to interest rates. Although the categories are fixed, the interest rate can slightly change within each category over the time. \n",
        "\n",
        "The term is calculated using the amount and the interest rate.\n",
        "\n",
        "It follows that `int_rate`, `grade`, `sub_grade` are correlated. Furthermore `loan_amnt`, `int_rate` and `term` define the `installment`. These relationships would bring collinearity into our model for features selection.\n",
        "\n",
        "We will therefore work with `loan_amnt`, `grade`, `sub_grade` and `term`. If it comes out that these features are important for predicting charge-off, we will conclude that their related variables `int_rate`, and `installment` are also important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJzc223xqW8-"
      },
      "source": [
        "not_predictor += ['int_rate', 'installment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw80hK4EqW9m"
      },
      "source": [
        "As far as FICO is concerned, there are 6 variables. Four of them are determined at the initial loan application, thus we can use them. It doesn't seem that they are updated. \n",
        "\n",
        "These 2 are significant and collinear, so only one needs to be selected. We choose\n",
        "\n",
        "    - fico_range_high\n",
        "    - fico_range_low\n",
        "\n",
        "These 2 are not so significant and, we believe are used for joint applications.\n",
        "\n",
        "    - sec_app_fico_range_high\n",
        "    - sec_app_fico_range_low\n",
        "\n",
        "\n",
        "However, These two are created, and undoubtedly updated, throughout the loan life. They should not be used:\n",
        "\n",
        "    - last_fico_range_high\n",
        "    - last_fico_range_low"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpt-ULXyqW9w"
      },
      "source": [
        "not_predictor += ['sec_app_fico_range_high', 'sec_app_fico_range_low', 'last_fico_range_high', 'last_fico_range_low']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wWgL0Q7DOY"
      },
      "source": [
        "As our EDA has shown, information related to gender, race, zip code and state can lead to some unfair handling by a predictive model. We will assume here that removing them as predictor would solve this issue. We further assume that the remaining predictors will not be correlated in any way with census information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVDGTDi_7DOm"
      },
      "source": [
        "not_predictor += ['addr_state','zip_code','Population', 'median_income_2016', \n",
        "               'female_pct', 'male_pct', \n",
        "               'Black_pct', 'White_pct', 'Native_pct', 'Asian_pct', 'Hispanic_pct', \n",
        "               'household_family_pct', 'poverty_level_below_pct', 'Graduate_Degree_pct', 'employment_2016_rate']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do07AxVcqW-O"
      },
      "source": [
        "#### **Correlation and redundancy**\n",
        "\n",
        "Features-pairs which correlate by either -1 or +1 can be considered to be redundant. However High absolute correlation does not imply redundancy of features in the context of classification, see textbook [4] . Therefore we will have a closer look at each correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n38drOx5qW-Y"
      },
      "source": [
        "# source code adapted from [5]\n",
        "def find_high_correlated_features(frame):\n",
        "    new_corr = frame.corr()\n",
        "    new_corr.loc[:,:] = np.tril(new_corr, k=-1) \n",
        "    new_corr = new_corr.stack()\n",
        "    print(new_corr[(new_corr > 0.80) | (new_corr < -0.80)])\n",
        "    \n",
        "predictor = list(set(df_loan_accepted_census_cleaned.columns.values)-set(not_predictor))\n",
        "find_high_correlated_features(df_loan_accepted_census_cleaned[predictor])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPmKHEGrqW_W"
      },
      "source": [
        "not_predictor += ['fico_range_low','num_rev_tl_bal_gt_0', 'num_actv_rev_tl','open_il_12m','open_rv_12m','avg_cur_bal','tot_hi_cred_lim','num_bc_tl']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUgk3nNZqW_u"
      },
      "source": [
        "`fico_range_low` and `fico_range_low` are highly correlated. We keep the high value.\n",
        "\n",
        "The Number of revolving trades with balance >0 `num_rev_tl_bal_gt_0` and the Number of currently active revolving trades `num_actv_rev_tl` are highly correlated with the Number of currently active bankcard accounts `num_actv_bc_tl`. It is safe to remove the formers for our classification task for identifying fully paid loans from charged-off ones.\n",
        "\n",
        "The number of open credit lines `open_acc` (preapproved loans between a financial institution and borrower that may be used repeatedly up to a certain limit and can subsequently be paid back prior to payments coming due) in the borrower's credit file is highly correlated with the Number of satisfactory accounts (good standing accounts that have been paid in full and on time) `num_sats`. We will keep both features.\n",
        "\n",
        "`open_il_24m` and `open_il_12m` are the Number of installment accounts opened in past 24 and 12 months respectively. Both values are strongly correlated. We will consider 24 months period since it includes 12 months period. We will handle `open_rv_12m` and `open_rv_24m` in the same way (Number of revolving trades opened in past 12, resp. 24 months).\n",
        "\n",
        "`tot_cur_bal`, `avg_cur_bal` Total and average current balance of all accounts are strongly related to `tot_hi_cred_lim`, Total high credit/credit limit. We will only keep the total current balance.\n",
        "\n",
        "`num_rev_accts` Number of revolving accounts (account created by a lender to represent debts where the outstanding balance does not have to be paid in full every month by the borrower to the lender) and `num_bc_tl` Number of bankcard accounts are correlated. This is because credit cards are usually considered as revolving accounts. We assume that the number of revolving accounts better describe the risk for loans and we will remove the number of bankcard accounts.\n",
        "\n",
        "`total_bal_ex_mort` Total credit balance excluding mortgage and `total_il_high_credit_limit` Total installment high credit/credit limit are correlated. With an installment account, the borrower pays back the loan plus interest by making regular payments for a fixed amount of time. We will keep both features.\n",
        "\n",
        "`bc_open_to_buy` Total open to buy on revolving bankcards (credit cards) can be considered as a subset of `total_bc_limit` Total bankcard high credit/credit limit, but both information can differ in many situations. We keep both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4elE8omqXAC"
      },
      "source": [
        "**Features values distribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtnUtRBlqXAK"
      },
      "source": [
        "For each remaining feature, we will plot the distribution of their values in both charged-off and fully paid categories. This will help us seing how they might impact the decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqCqav-2qXAS"
      },
      "source": [
        "nb = 1\n",
        "for var in [x for x in df_loan.columns.values if x not in not_predictor]:\n",
        "    if df_loan[var].dtype == np.float64 or df_loan[var].dtype == np.int64:\n",
        "        nb = nb + 1\n",
        "fig, ax = plt.subplots(nb//2, 2, figsize=(15,90))\n",
        "i = 0\n",
        "for var in [x for x in df_loan.columns.values if x not in not_predictor]:\n",
        "    if df_loan[var].dtype == np.float64 or df_loan[var].dtype == np.int64:\n",
        "        sns.kdeplot(df_loan[df_loan.loan_status=='Charged Off'][var], label='Charged Off', ax=ax[i//2,i % 2])\n",
        "        sns.kdeplot(df_loan[df_loan.loan_status!='Charged Off'][var], label='Fully Paid', ax=ax[i//2,i % 2])\n",
        "        ax[i//2,i % 2].set_ylabel(var)\n",
        "        i = i + 1\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwS9iZUfqXA-"
      },
      "source": [
        "Looking at the plots above we see that the distribution for census-related features is almost the same accross both classes of loans. We will investigate those features closer using mode-based features selection in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtq1BFJqXBE"
      },
      "source": [
        "**One-hot encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FhRMinoqXBI"
      },
      "source": [
        "We turn the loan status into a binary variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvR_zWEqXBO"
      },
      "source": [
        "df_loan.replace({'loan_status':{'Charged Off': 1, 'Fully Paid': 0}}, inplace=True)\n",
        "df_loan.loan_status = df_loan.loan_status.astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqbtniVMqXB4"
      },
      "source": [
        "We convert the `earliest_cr_line` feature to the number of years between the earliest credit line and the year when the loan was requested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtAu4GwSqXCA"
      },
      "source": [
        "df_loan.earliest_cr_line = pd.to_datetime(df_loan.issue_d, format='%b-%Y').dt.to_period('Y') - pd.to_datetime(df_loan.earliest_cr_line, format='%b-%Y').dt.to_period('Y')\n",
        "df_loan.earliest_cr_line = df_loan.earliest_cr_line.astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MutOeU9BqXCw"
      },
      "source": [
        "We turn categorical features into binary variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk2PhhSOU6W2",
        "scrolled": true
      },
      "source": [
        "df_loan.replace({'term':{36: 1, 60: 0}},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF75nmaMqXDO"
      },
      "source": [
        "df_loan = pd.get_dummies(df_loan, columns=['emp_length', 'home_ownership','purpose','issue_m',\n",
        "                                               'grade', 'sub_grade','earliest_cr_line'], drop_first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHHTCFARqXDy"
      },
      "source": [
        "**Remove irrelevant features**\n",
        "\n",
        "Let's remove all indentified features above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDs8UV3lqXD6",
        "scrolled": true
      },
      "source": [
        "df_loan.drop(columns=list(set(not_predictor) & set(df_loan.columns.values)), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Aw5H8wmqXEc"
      },
      "source": [
        "### Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxpSdmqa27YO"
      },
      "source": [
        "As we see below, the data is unbalanced, with Fully Paid loans being the majority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3k860R5aSsA",
        "scrolled": false
      },
      "source": [
        "nb = df_loan.loan_status.value_counts()\n",
        "nb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NU5puJqBUGD"
      },
      "source": [
        "Here, 0 - Fully Paid loans and 1 - Charged Off loans. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed53Iq907DRQ"
      },
      "source": [
        "print(\"We notice, {0:.2%} of the total loans are Charged Off, and about {1:.2%} loans are Fully Paid.\".format(nb[0]/np.sum(nb),nb[1]/np.sum(nb)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEz5uImJBnik"
      },
      "source": [
        "**Train/Test/Validation Data Split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TQ4nQRlqXFM"
      },
      "source": [
        "`X` is the feature matrix. `Y` is the response vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-5fdZEEqXFQ"
      },
      "source": [
        "X, Y = df_loan[df_loan.columns.difference(['loan_status'])], df_loan['loan_status']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TO6QBwOqXFe"
      },
      "source": [
        "We choose to split the whole dataset to 90% training, 10% test, with stratify, resulting in consistent class distribution between training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geiSxAL6O9TY"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, stratify=Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeS4YY5zqXF4"
      },
      "source": [
        "Let's further split the training set into a 80% training and a 20% validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIkd_6VhqXF-"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9r6gq6p7DRu"
      },
      "source": [
        "print(\"We verify that the proportion of Charged Off is about the same: {0:.2%} in train, {1:.2%} in val and {2:.2%} in test\".format(len(y_train[y_train==0])/len(y_train),\n",
        "                                                                                                                                   len(y_val[y_val==0])/len(y_val),\n",
        "                                                                                                                                   len(y_test[y_test==0])/len(y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs6_OSU4qXGO"
      },
      "source": [
        "We will now have a closer look at the imbalanced classes.\n",
        "\n",
        "Let's reduce the dimension of a subset of the data using Principal Component Analysis (PCA) and display the imbalanced classes in a 2D plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZbomD8mqXGU"
      },
      "source": [
        "X_train_subset = X_train[:100]\n",
        "y_train_subset = y_train[:100]\n",
        "pca = PCA(n_components=2)\n",
        "X_train_subset = pca.fit_transform(X_train_subset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gRHfHE_qXGs"
      },
      "source": [
        "y_train_subset.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTHQ7iv6qXHC"
      },
      "source": [
        "# source code adapted from [6]\n",
        "def plot_2d_space(X, y, label='Classes'):   \n",
        "    colors = ['#1F77B4', '#FF7F0E']\n",
        "    markers = ['o', 's']\n",
        "    labels = ['Charged Off', 'Fully Paid']\n",
        "    for d, l, c, m in zip(labels, np.unique(y), colors, markers):\n",
        "        plt.scatter(\n",
        "            X[y==l, 0],\n",
        "            X[y==l, 1],\n",
        "            c=c, label=d, marker=m, alpha=.5\n",
        "        )\n",
        "    plt.title(label)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel('PCA Dimension 1')\n",
        "    plt.ylabel('PCA Dimension 2')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haBmxcPAqXHg"
      },
      "source": [
        "plot_2d_space(X_train_subset, y_train_subset, 'Imbalanced dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efP1s4G8qXH0"
      },
      "source": [
        "The plot above is a visual confirmation of the imbalanced classes in our data.\n",
        "\n",
        "A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of either removing samples from the majority class (under-sampling) or adding more examples from the minority class (over-sampling). Both strategies can also be applied at the same time.\n",
        "\n",
        "As shown below with under-sampling, we tend to loose valuable information, which can increase bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEvAesE_qXH-"
      },
      "source": [
        "rus = RandomUnderSampler(return_indices=True)\n",
        "X_train_subset_rus, y_train_subset_rus, id_rus = rus.fit_sample(X_train_subset, y_train_subset)\n",
        "print(X_train_subset.shape[0] - X_train_subset_rus.shape[0], 'random samples removed')\n",
        "plot_2d_space(X_train_subset_rus, y_train_subset_rus, 'Random under-sampling')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFi43wDHqXIO"
      },
      "source": [
        "In over-sampling the most naive strategy is to generate new samples by randomly sampling with replacement the current available samples, which can cause overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQw9KEZMqXIU"
      },
      "source": [
        "ros = RandomOverSampler()\n",
        "X_train_subset_ros, y_train_subset_ros = ros.fit_sample(X_train_subset, y_train_subset)\n",
        "print(X_train_subset_ros.shape[0] - X_train_subset.shape[0], 'random samples added')\n",
        "plot_2d_space(X_train_subset_ros, y_train_subset_ros, 'Random over-sampling')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOUm77jTqXIq"
      },
      "source": [
        "A number of more sophisticated resapling techniques have been proposed in the scientific literature, especially using the Python library imbalanced-learn [7]. SMOTE (Synthetic Minority Oversampling TEchnique) consists of creating new samples for the minority class, by picking a sample from that class and computing the k-nearest neighbors, then adding a new sample between the chosen sample and its neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4C5o9YuqXIu"
      },
      "source": [
        "smote = SMOTE(ratio='minority')\n",
        "X_train_subset_sm, y_train_subset_sm = smote.fit_sample(X_train_subset, y_train_subset)\n",
        "\n",
        "plot_2d_space(X_train_subset_sm, y_train_subset_sm, 'SMOTE over-sampling')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsKyF2-DqXJA"
      },
      "source": [
        "**We will use SMOTE to balance our training dataset.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV8JAvhKqXJE"
      },
      "source": [
        "X_train, y_train = smote.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bb8Z6jnqXJS"
      },
      "source": [
        "print('The Charged-Off to Fully Paid ratio in the balanced training set is now: ', len(y_train[y_train==0])/len(y_train[y_train==1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQrx-DCQqXKA"
      },
      "source": [
        "### Model-based features selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Xgw4NdqXM6"
      },
      "source": [
        "**Insights on features importance**\n",
        "\n",
        "Looking at the results above, we can bring in following conclusions:\n",
        "\n",
        "- The term is the most important element each investor has to care about. 68-months loans are highly risky.\n",
        "- The purpose of loan for credit cards payment brings more confidence to an investor.\n",
        "- Borrowers who have 10 or more years verified working experience are the most trustworthy investment.\n",
        "- Home ownership plays a significant role.\n",
        "- The state of California is a significant factor to be considered when looking at the likelihood of Charged-Off\n",
        "- Lenders should look at financial KPIs such as inq_last_6mths, num_tl_op_past_12m and acc_open_past_24mths; not just at FICO score, which are less relevant than these KPIs.\n",
        "- Debt-to-income ratio and annual income can be missleading, and shoudn't be always considered as the most important factors.\n",
        "- The time of the year when the loan is requested is not so relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNHCm7HUqXKE"
      },
      "source": [
        "\n",
        "We will use classifiers on the training dataset in order to get a better understanding on how features are related to loan outcome as fully paid or unpaid. This will help us reducing the dimensionality of our data by selecting the most important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbk4cmTSqXK2"
      },
      "source": [
        "randomf = RandomForestClassifier(n_estimators=100, max_depth=None, oob_score=True).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vviHR8g17DTm"
      },
      "source": [
        "print('Accuracy, Training Set: {0:0.2%}'.format(randomf.score(X_train, y_train)))\n",
        "print('OOB Score, Training Set: {0:0.2%}'.format(randomf.oob_score_))\n",
        "print('Accuracy, Validation Set: {0:0.2%}'.format(randomf.score(X_val, y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oQQloxsqXLu"
      },
      "source": [
        "The random forest is clearly overfit, and is obviously picking the majority class in the validation set.\n",
        "\n",
        "Below we have a ranking of features as computed by our random forest, depending on their Gini importance in the prediction of loan outcome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjR1pbAqXLy"
      },
      "source": [
        "feature_importances = pd.DataFrame(randomf.feature_importances_,\n",
        "                                   index = X.columns,\n",
        "                                    columns=['importance']).sort_values('importance', ascending=False).reset_index().rename(columns={'index':'feature'})\n",
        "feature_importances.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1ZNhcHsqXMC"
      },
      "source": [
        "**Important Features**\n",
        "\n",
        "We will now use SKLearn meta-transformet SelectFromModel to discard irrelevant features using the features importance produced by our random forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDruq6aLqXMG"
      },
      "source": [
        "fs_model = SelectFromModel(randomf, prefit=True)\n",
        "outcome = fs_model.get_support()\n",
        "features_list_orig = X.columns.values\n",
        "features_list_new = []\n",
        "for i in range(0,len(features_list_orig)):\n",
        "    if outcome[i]:\n",
        "        features_list_new.append(features_list_orig[i])\n",
        "print('{} features were selected from the {} original hot-encoded ones:'.format(len(features_list_new), len(features_list_orig)))\n",
        "print('')\n",
        "print(features_list_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-aWwHHkqXMq",
        "scrolled": true
      },
      "source": [
        "loan_variables_selected = []\n",
        "for col in df_loan_accepted_census_cleaned.columns:\n",
        "    if len([s for s in features_list_new if s.startswith(col)])>0:\n",
        "        loan_variables_selected.append(col)\n",
        "\n",
        "print('After hot-decoding, they corresponds to the following {} features from the original dataset.'.format(len(loan_variables_selected)))\n",
        "print('')\n",
        "print(loan_variables_selected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A16OPHrCqXM-"
      },
      "source": [
        "**Design Matrix with important features**\n",
        "\n",
        "We can now create the new design matrix using the identified important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdM21X2k7DUo"
      },
      "source": [
        "print(\"Number of predictors after one-hot encoding is: \",X_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfmetIeXqXNC"
      },
      "source": [
        "X_train = fs_model.transform(X_train)\n",
        "X_val = fs_model.transform(X_val)\n",
        "X_test = fs_model.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65RDFDnvWQ0m"
      },
      "source": [
        "print(\"Number of predictors after feature selection is \", X_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL__KcEmqXNS"
      },
      "source": [
        "**Principal Components Analysis**\n",
        "\n",
        "We are interested in reducing the dimension of our data further by analysing its principal components. This will allow us to compress the important features into a reduced number of components.\n",
        "\n",
        "We first start with scaling the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER_fA_V5qXNW"
      },
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz91lFKuqXNi"
      },
      "source": [
        "We decompose the scaled data with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpgS0Sb_qXNm"
      },
      "source": [
        "n = X_train.shape[1]\n",
        "pca_fit = PCA(n).fit(X_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1p4Cc0RqXOM"
      },
      "source": [
        "In the plot below, we see that the dimension can be reduced to the number of components which explain at least 90% of variance in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuoldbYsqXOS",
        "scrolled": false
      },
      "source": [
        "pca_var = np.cumsum(pca_fit.explained_variance_ratio_)\n",
        "plt.scatter(range(1,n+1),pca_var)\n",
        "plt.xlabel(\"PCA Dimensions\")\n",
        "plt.ylabel(\"Total Variance Captured\")\n",
        "plt.title(\"Variance Explained by PCA\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVXjPpSuqXPA"
      },
      "source": [
        "We can now rebuild our design matrix using the PCA components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGx9lpE1qXPG"
      },
      "source": [
        "pca_fit = PCA(25).fit(X_train_scaled)\n",
        "X_train_pca = pca_fit.transform(X_train)\n",
        "X_val_pca = pca_fit.transform(X_val)\n",
        "X_test_pca = pca_fit.transform(X_test)\n",
        "X_train_scaled_pca = pca_fit.transform(X_train_scaled)\n",
        "X_val_scaled_pca = pca_fit.transform(X_val_scaled)\n",
        "X_test_scaled_pca = pca_fit.transform(X_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75l_-z6N7DVs"
      },
      "source": [
        "**Helper function for Design Matrix**\n",
        "\n",
        "We summarize the above code in the function below, which create all design matrices need for our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2j_6jB7DVs"
      },
      "source": [
        "def get_design_matrices(df, variables = ['loan_amnt', 'term', 'sub_grade', 'grade', 'emp_length', 'home_ownership',\n",
        "           'annual_inc', 'loan_status', 'purpose', 'dti', 'delinq_2yrs',\n",
        "           'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq',\n",
        "           'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
        "           'revol_util', 'total_acc', 'collections_12_mths_ex_med',\n",
        "           'mths_since_last_major_derog', 'acc_now_delinq', 'tot_coll_amt',\n",
        "           'tot_cur_bal', 'total_rev_hi_lim', 'acc_open_past_24mths',\n",
        "           'bc_open_to_buy', 'bc_util', 'delinq_amnt', 'mo_sin_old_il_acct',\n",
        "           'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\n",
        "           'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
        "           'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n",
        "           'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_bc_sats',\n",
        "           'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_sats',\n",
        "           'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',\n",
        "           'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75',\n",
        "           'pub_rec_bankruptcies', 'tax_liens', 'total_bal_ex_mort',\n",
        "           'total_bc_limit', 'total_il_high_credit_limit','earliest_cr_line']):\n",
        "    \n",
        "    # raw data\n",
        "    df_loan = df.copy()\n",
        "    df_loan = df_loan[df_loan.loan_status.isin(['Charged Off', 'Fully Paid'])]\n",
        "    df_loan = df_loan[variables]\n",
        "    # hot encoding\n",
        "    df_loan.replace({'loan_status':{'Charged Off': 1, 'Fully Paid': 0}}, inplace=True)\n",
        "    df_loan.loan_status = df_loan.loan_status.astype('int')\n",
        "    df_loan.replace({'term':{36: 1, 60: 0}},inplace=True)\n",
        "    df_loan = pd.get_dummies(df_loan, columns=['emp_length', 'home_ownership','purpose',\n",
        "                                               'grade', 'sub_grade','earliest_cr_line'], drop_first=True)\n",
        "    # design matrix\n",
        "    X, Y = df_loan[df_loan.columns.difference(['loan_status'])], df_loan['loan_status']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, stratify=Y)\n",
        "    X_test_df = X_test\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n",
        "    # upsampling\n",
        "    smote = SMOTE(ratio='minority')\n",
        "    X_train, y_train = smote.fit_sample(X_train, y_train)\n",
        "    X_val, y_val = smote.fit_sample(X_val, y_val)\n",
        "    # features selection\n",
        "    randomf = RandomForestClassifier(n_estimators=100, max_depth=None, oob_score=True).fit(X_train, y_train)\n",
        "    fs_model = SelectFromModel(randomf, prefit=True)\n",
        "    X_train = fs_model.transform(X_train)\n",
        "    X_val = fs_model.transform(X_val)\n",
        "    X_test = fs_model.transform(X_test)\n",
        "    # scaling\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    # pca transform\n",
        "    pca_fit = PCA(.9).fit(X_train_scaled)\n",
        "    X_train_pca = pca_fit.transform(X_train)\n",
        "    X_val_pca = pca_fit.transform(X_val)\n",
        "    X_test_pca = pca_fit.transform(X_test)\n",
        "    X_train_scaled_pca = pca_fit.transform(X_train_scaled)\n",
        "    X_val_scaled_pca = pca_fit.transform(X_val_scaled)\n",
        "    X_test_scaled_pca = pca_fit.transform(X_test_scaled)\n",
        "    return X_test_df, X_train, y_train, X_val, y_val, X_test, y_test, X_train_scaled, X_val_scaled, X_test_scaled, X_train_scaled_pca, X_val_scaled_pca, X_test_scaled_pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKsEugl5j5Zq"
      },
      "source": [
        "X_test_df, X_train, y_train, X_val, y_val, X_test, y_test, X_train_scaled, X_val_scaled, X_test_scaled,X_train_scaled_pca, X_val_scaled_pca, X_test_scaled_pca = get_design_matrices(df_loan_accepted_census_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3aL8F3UqXPQ"
      },
      "source": [
        "## Classification Models\n",
        "\n",
        "In the previous section we used a model-based approach for identifying the important features which most probably determine the failure or success of a loan.\n",
        "\n",
        "Using the features selected, we will now investigate the performance of several models on the validation set via cross-validation. Each model will be trained on the training set. We will also check if dimension reduction via PCA improves the accuracy. \n",
        "\n",
        "At the end we will investigate if ensemble technique via stacking of base learners improves classification results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PixV8ubZqXPi"
      },
      "source": [
        "\n",
        "### Scoring\n",
        "\n",
        "In classification problems, we can distinguish between the following metrics, whereby **the positive class is Charge Off** and **the negative class is Fully Paid**:\n",
        "\n",
        "- **Recall or Sensitivity or TPR (True Positive Rate)**: Number of loans correctly identified as positive (fully paid) out of total true positives - TP/(TP+FN)\n",
        "    \n",
        "- **Specificity or TNR (True Negative Rate)**: Number of loans correctly identified as negative (charged-off) out of total negatives - TN/(TN+FP)\n",
        "\n",
        "- **Precision**: Number of loans correctly identified as positive (fully paid) out of total items identified as positive - TP/(TP+FP)\n",
        "    \n",
        "- **False Positive Rate or Type I Error**: Number of loans wrongly identified as positive (fully paid) out of total true negatives - FP/(FP+TN)\n",
        "    \n",
        "- **False Negative Rate or Type II Error**: Number of loans wrongly identified as negative (charged-off) out of total true positives - FN/(FN+TP)\n",
        "\n",
        "- A **Confusion Matrix**: visual representation of the number of TP, TN, FP and FN.\n",
        "\n",
        "- **Accuracy**: Percentage of total items classified correctly - (TP+TN)/(N+P)\n",
        "\n",
        "- **F1 Score**: Harmonic mean of precision and recall given by - F1 = 2xPrecisionxRecall /(Precision + Recall)\n",
        "\n",
        "- **ROC-AUC Score**: Area under curve of sensitivity (TPR) vs. specificity (TNR).\n",
        "\n",
        "- **Log-loss**: Probabilistic confidence of accuracy. High value of log-loss means that the absolute probabilities have big difference from actual labels.  \n",
        "\n",
        "**Scoring in investment strategy**\n",
        "\n",
        "Insights from [8] were used in this section.\n",
        "\n",
        "If we choose an investment strategy that uses absolute probabilistic difference, then we will  look at log-loss with care. If the final class prediction is the most important outcome and we don’t want to tune probability threshold, we will rather use AUC score. But if the threshold is well tuned, then F1 will be the scoring to use.\n",
        "\n",
        "In loan classification, where positive labels (charged-offs) are few, we would like our model to predict positive classes correctly and hence we will sometime prefer those models which are able to classify these positive labels. Log-loss usually fails to identify model which produces too many false negatives because the log-loss function is symmetric and does not differentiate between classes.  Both F1 score and ROC-AUC score can perform well for class imbalance. F1 is better suit for situations where the negative class is small. Since an investor would care more about the minority class (charged-off loans) in number independent of the fact whether it is positive or negative, then **we think that ROC-AUC score would make sense as benchmark measure.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq-zko4fqXPm"
      },
      "source": [
        "**Helper functions for scoring metrics**\n",
        "\n",
        "The source code from [9] was adjusted in the functions below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls0GyqK3qXPo"
      },
      "source": [
        "# dataframe where we track all cross-validation scoring metrics\n",
        "df_cv_scores = pd.DataFrame({'model':['dummy'], 'accuracy':[0], 'f1':[0], 'roc_auc':[0]}, \n",
        "                            columns=['accuracy','f1','roc_auc'], index=['model'])\n",
        "df_cv_scores_pca = df_cv_scores.copy()\n",
        "\n",
        "df_y_pred_probas = pd.DataFrame() \n",
        "df_y_preds = pd.DataFrame() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMjGjeF27DWG"
      },
      "source": [
        "# function adjusts class predictions based on the prediction threshold\n",
        "def adjust_proba(probs, threshold):\n",
        "    return [1 if proba >= threshold else 0 for proba in probs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHbxof9_qXP4"
      },
      "source": [
        "# function for computing 5-fold cross-validation scoring scores\n",
        "def predict_evaluate_cv(model, X, y, df_cv_scores):\n",
        "    cv = StratifiedKFold(n_splits=3, random_state=9999) \n",
        "    score_accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    score_f1 = cross_val_score(model, X, y, cv=cv, scoring='f1').mean()\n",
        "    score_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    df_cv_scores.loc[model.__class__.__name__] = [score_accuracy, score_f1, score_auc]\n",
        "    print('K-fold cross-validation results on validation set:')\n",
        "    print(\" average accuracy is {0:0.2%}\".format(score_accuracy))\n",
        "    print(\" average F1 is {0:0.2%}\".format(score_f1))\n",
        "    print(\" average roc_auc is {0:0.2%}\".format(score_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4_-ZnmyqXQO"
      },
      "source": [
        "# function for computing the confusion matrix\n",
        "def predict_evaluate_cm(model, X, y, threshold=.5):\n",
        "    model_name = model.__class__.__name__\n",
        "    classes = ['Fully Paid', 'Charged-Off']\n",
        "    y_true = y\n",
        "    if model_name == 'SVC':\n",
        "        y_pred_proba = model.decision_function(X)\n",
        "    else:\n",
        "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "    df_y_pred_probas[model_name] = y_pred_proba\n",
        "    y_pred = adjust_proba(y_pred_proba, threshold)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm = cm.astype('float')\n",
        "    np.set_printoptions(precision=2)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"gray\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.grid(False)\n",
        "    plt.rcParams[\"axes.edgecolor\"] = \"0.85\"\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsQXEx7yqXQY"
      },
      "source": [
        "# function for compution the roc plot\n",
        "def predict_evaluate_roc(model, X, y, threshold=.5):\n",
        "    y_pred = model.predict(X)\n",
        "    model_name = model.__class__.__name__\n",
        "    df_y_preds[model_name] = y_pred\n",
        "    if model_name == 'SVC':\n",
        "        y_pred_proba = model.decision_function(X)\n",
        "    else:\n",
        "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
        "    [fpr, tpr, thr] = roc_curve(y, y_pred_proba)\n",
        "\n",
        "    idx = np.min(np.where(tpr > threshold))\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\n",
        "    plt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
        "    plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
        "    plt.title('Receiver operating characteristic (ROC) curve - {}'.format(model_name))\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n",
        "          \"and a specificity of %.3f\" % (1-fpr[idx]) + \n",
        "          \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUhkFVm0qXRS"
      },
      "source": [
        "# global function for fitting, cross-validating and evaluating a given classifier\n",
        "def fit_predict_evaluate(model, Xtrain, ytrain, Xval, yval, df_cv_scores):\n",
        "    model.fit(Xtrain, ytrain)\n",
        "    print(model.__class__.__name__+\":\")\n",
        "    print('Accuracy score on training set is {0:0.2%}'.format(model.score(Xtrain, ytrain)))\n",
        "    predict_evaluate_cv(model, Xval, yval, df_cv_scores)\n",
        "    predict_evaluate_cm(model, Xval, yval)\n",
        "    predict_evaluate_roc(model, Xval, yval)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxbZLPLbj5aM"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnCOe0l8j5aM"
      },
      "source": [
        "dt_model = DecisionTreeClassifier(max_depth = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvtfv8X-j5aU"
      },
      "source": [
        "dt_model = fit_predict_evaluate(dt_model, X_train, y_train, X_val, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC9T_YvLqXRc"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "We will start with a simple logistic regression model for predicting loan charge-off. The penalty parameter is found via cross-validation on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbh8VjNZqXRg"
      },
      "source": [
        "log_reg = LogisticRegressionCV(\n",
        "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
        "        ,penalty='l2'\n",
        "        ,scoring='roc_auc'\n",
        "        ,cv=5\n",
        "        ,random_state=777\n",
        "        ,max_iter=10000\n",
        "        ,fit_intercept=True\n",
        "        ,solver='newton-cg'\n",
        "        ,tol=10\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD74MliSqXRw"
      },
      "source": [
        "log_reg = fit_predict_evaluate(log_reg, X_train_scaled, y_train, X_val_scaled, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67_yOSZwqXSC"
      },
      "source": [
        "**Logistic Regression with PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKXzDWwQj5a0"
      },
      "source": [
        "log_reg_pca = fit_predict_evaluate(log_reg, X_train_scaled_pca, y_train, X_val_scaled_pca, y_val, df_cv_scores_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT96DcRDqXSW"
      },
      "source": [
        "PCA causes a decrease in average AUC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2TEWF4eqXSY"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "We will now rebuilt our random forest classifier, this time using the important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnMhmYH7qXSu"
      },
      "source": [
        "randomf_optim = RandomForestClassifier(n_estimators=200, max_depth=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvFkla2PqXS4",
        "scrolled": false
      },
      "source": [
        "randomf_optim = fit_predict_evaluate(randomf_optim, X_train, y_train, X_val, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6ezEU3rqXTO"
      },
      "source": [
        "The random forest classifier gives the best accuracy so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9rqKXmSqXTS"
      },
      "source": [
        "## Boosting\n",
        "\n",
        "In this section, we try the boosting technique for loan default prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DpmK2rHqXTW"
      },
      "source": [
        "ab_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=7), n_estimators=60, learning_rate=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ko-43vJqXTo",
        "scrolled": true
      },
      "source": [
        "ab_model = fit_predict_evaluate(ab_model, X_train, y_train, X_val, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTu6Sm-vqXT2",
        "scrolled": true
      },
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
        "train_scores = list(ab_model.staged_score(X_train,y_train))\n",
        "test_scores = list(ab_model.staged_score(X_val, y_val))\n",
        "ax[0].plot(train_scores,label='depth-{}'.format(7))\n",
        "ax[1].plot(test_scores,label='depth-{}'.format(7))\n",
        "ax[0].set_xlabel('number of iterations', fontsize=12)\n",
        "ax[1].set_xlabel('number of iterations', fontsize=12)\n",
        "ax[0].set_ylabel('Accuracy', fontsize=12)\n",
        "ax[0].set_title(\"Variation of Accuracy with Iterations (training set)\", fontsize=14)\n",
        "ax[1].set_title(\"Variation of Accuracy with Iterations (validation set)\", fontsize=14)\n",
        "ax[0].legend(fontsize=12);\n",
        "ax[1].legend(fontsize=12);    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBizde3qqXUA"
      },
      "source": [
        "### XG Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0MsB0GjqXUE"
      },
      "source": [
        "xgb_model = XGBClassifier(learningrate =0.05, nestimators=100,\n",
        "                    maxdepth=4, minchildweight=4, subsample=0.8, colsamplebytree=0.8,\n",
        "                    objective= 'binary:logistic',\n",
        "                    nthread=3,\n",
        "                    scaleposweight=2,\n",
        "                    seed=27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDqf5WeU7DW-"
      },
      "source": [
        "xgb_model = fit_predict_evaluate(xgb_model, X_train, y_train, X_val, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSgnnV8jqXUm"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfvIrh-NqXUo"
      },
      "source": [
        "svm_model = SVC(gamma=0.1, C=0.01, kernel=\"linear\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPRRlyL8qXUy"
      },
      "source": [
        "svm_model = fit_predict_evaluate(svm_model, X_train_scaled, y_train, X_val_scaled, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjFi8irfqXU-"
      },
      "source": [
        "## QDA\n",
        "\n",
        "In this section we try a QDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5A3a3YKqXVA"
      },
      "source": [
        "qda_model = QuadraticDiscriminantAnalysis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76uG4FdoqXVK"
      },
      "source": [
        "qda_model = fit_predict_evaluate(qda_model, X_train_scaled, y_train, X_val_scaled, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EltAZAGW7DXQ"
      },
      "source": [
        "qda_model_pca = fit_predict_evaluate(qda_model, X_train_scaled_pca, y_train, X_val_scaled_pca, y_val, df_cv_scores_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLdCoVtXqXVW"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqoOmU2-qXVY"
      },
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewVkmRM27DXk"
      },
      "source": [
        "knn_model = fit_predict_evaluate(knn_model, X_train_scaled, y_train, X_val_scaled, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZStiekLxqXVg",
        "scrolled": true
      },
      "source": [
        "knn_model_pca = fit_predict_evaluate(knn_model, X_train_scaled_pca, y_train, X_val_scaled_pca, y_val, df_cv_scores_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAn6qg6rj5cC"
      },
      "source": [
        "## Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SjvyQUBj5cC"
      },
      "source": [
        "mlp_model = MLPClassifier(hidden_layer_sizes=(50), batch_size=50, learning_rate='constant', learning_rate_init=0.0005, early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdJJAy7xj5cE"
      },
      "source": [
        "mlp_model = fit_predict_evaluate(mlp_model, X_train_scaled, y_train, X_val_scaled, y_val, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXkYMVC-j5cI"
      },
      "source": [
        "mlp_model_pca = fit_predict_evaluate(mlp_model, X_train_scaled_pca, y_train, X_val_scaled_pca, y_val, df_cv_scores_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQxDe4itqXVo"
      },
      "source": [
        "## Loan Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjpOD-fSqXVs"
      },
      "source": [
        "Some studies [10] suggest that words used on loan applications can predict the likelihood of charge-off.\n",
        "In this section we use natural language processing algorithms to extract features from the loan title and description filled in by the borrower when requesting the loan. We then use a Naive Bayes classifier and a random forest for this task.\n",
        "\n",
        "The function below performs the following preprocessingon the text data:\n",
        "\n",
        "- lower case\n",
        "- remove punctuation\n",
        "- remove commonly occuring words using predefined libraries\n",
        "- remove suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgMyKu81qXVu"
      },
      "source": [
        "# source code adapted from [11]\n",
        "def clean_text(text):\n",
        "    # lower case\n",
        "    text = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "    # remove punctuation\n",
        "    text = text.str.replace('[^\\w\\s]','') \n",
        "    # remove stop words    \n",
        "    stop = stopwords.words('english')\n",
        "    text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "    # correct spelling\n",
        "    #from textblob import TextBlob\n",
        "    #text = text.apply(lambda x: str(TextBlob(x).correct()))\n",
        "    # lemmatization     \n",
        "    text = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLDtfSBlj5cO"
      },
      "source": [
        "We first prepare the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqf9vyxtqXWA",
        "scrolled": true
      },
      "source": [
        "df_desc = df_loan_accepted_census_cleaned.copy()\n",
        "df_desc = df_desc[['title', 'desc', 'loan_status']]\n",
        "df_desc.fillna('N/A', inplace=True)\n",
        "df_desc['desc'] = df_desc['desc'] + ' - ' + df_desc['title']\n",
        "df_desc = df_desc[df_desc.loan_status.isin(['Charged Off', 'Fully Paid'])]\n",
        "df_desc.replace({'loan_status':{'Charged Off': 0, 'Fully Paid': 1}}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1-mIjZuj5cQ"
      },
      "source": [
        "The feature is the merged loan title and description, the response variable is the loan status."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d021axqwqXWI",
        "scrolled": true
      },
      "source": [
        "X_desc = df_desc.title\n",
        "y_desc = df_desc.loan_status\n",
        "#X_train_desc, X_test_desc, y_train_desc, y_test_desc = train_test_split(X_desc, y_desc, test_size= 0.2, random_state=13)\n",
        "X_train_desc, X_test_desc, y_train_desc, y_test_desc = train_test_split(X_desc, y_desc, test_size=0.1, stratify=y_desc)\n",
        "X_train_desc, X_val_desc, y_train_desc, y_val_desc = train_test_split(X_train_desc, y_train_desc, test_size=0.2, stratify=y_train_desc)\n",
        "\n",
        "# upsampling\n",
        "X_train_desc, y_train_desc = resample(X_train_desc, y_train_desc, n_samples=round(y_train.shape[0]))\n",
        "X_val_desc, y_val_desc = resample(X_val_desc, y_val_desc, n_samples=round(y_val.shape[0]))\n",
        "X_test_desc, y_test_desc = resample(X_test_desc, y_test_desc, n_samples=round(y_test.shape[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSsvHZ-Cj5cU"
      },
      "source": [
        "We extract TF-IDF features. TF-IDF is the multiplication of the TF and IDF.  The IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.The TF or term frequency is the ratio of the count of a word present in a sentence, to the length of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFbdndJFqXWS"
      },
      "source": [
        "word_vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
        "    ngram_range=(1, 1),\n",
        "    max_features=30000)\n",
        "\n",
        "# fit and transform on it the training features\n",
        "word_vectorizer.fit(X_train_desc)\n",
        "X_train_desc_features = word_vectorizer.transform(X_train_desc)\n",
        "\n",
        "#transform the val and test features to sparse matrix\n",
        "X_val_desc_features = word_vectorizer.transform(X_val_desc)\n",
        "X_test_desc_features = word_vectorizer.transform(X_test_desc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QD_u92wqXWc"
      },
      "source": [
        "print(\"Our training data has {} loans, each with {} text features.\".format(X_train_desc_features.shape[0], \n",
        "                                                                           X_train_desc_features.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEQSxFh77DX4"
      },
      "source": [
        "**Naive Bayes**\n",
        "\n",
        "The multinomial Naive Bayes classifier is suitable for classification with discrete features and fractional counts such as tf-idf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGEcxtykqXWo"
      },
      "source": [
        "nb_model = MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTY0wzTIqXWy",
        "scrolled": true
      },
      "source": [
        "nb_model = fit_predict_evaluate(nb_model, X_train_desc_features, y_train_desc, X_val_desc_features, y_val_desc, df_cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJysZdo8qXXQ"
      },
      "source": [
        "Both Naive Bayes and random forest did not find enough information that would clearly distinguish the two classes of loans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46FRQWCd4-lm"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCF5EuaMqXZu"
      },
      "source": [
        "## Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PzHesY8j5ce"
      },
      "source": [
        "### Scoring benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEXXbSzAqXZw"
      },
      "source": [
        "All models investigated in this project are compared in the table below. The performance was evaluated in a out-of-sample manner on the validation set using 3-folds cross-validation.\n",
        "\n",
        "For each model, we can compare different metrics (accuracy, F1, ROC_AUC). We are also able to compare models using the metric ROC_AUC, which reflect their performance when we set probability cutoff at 50%. \n",
        "\n",
        "The scoring results is summarised below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UUW06nEqXZ0",
        "scrolled": true
      },
      "source": [
        "df_cv_scores_report = df_cv_scores.copy()\n",
        "df_cv_scores_report.drop('model', inplace=True, errors='ignore')\n",
        "df_cv_scores_report.sort_values(by=['roc_auc'], ascending=False, inplace=True)\n",
        "df_cv_scores_report.reset_index(inplace=True)\n",
        "df_cv_scores_report.rename(columns={'index':'model'}, inplace=True)\n",
        "df_cv_scores_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ipF1iggj5ci"
      },
      "source": [
        "df_cv_scores_melt = pd.melt(df_cv_scores_report, id_vars=['model'], var_name='metric', value_name='score')\n",
        "fig, ax = plt.subplots(1,1,figsize=(25,5))\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.barplot(x=\"model\", y=\"score\", hue=\"metric\", data=df_cv_scores_melt, palette=\"muted\", ax=ax)\n",
        "ax.set_title(\"Models Benchmark via Cross-Validation on Validation set\")\n",
        "ax.set_ylabel(\"Score\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Q2KQV5j5cm"
      },
      "source": [
        "Some models were also evaluated on a scaled version of the data (with features standardized with zero mean and standard deviation 1). The results below show that standardizing the data did not improve the prediction accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNHyVmy9rutB"
      },
      "source": [
        "# Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A68ThqrEj5cm"
      },
      "source": [
        "### Random Forest Classifier is the best performer\n",
        "\n",
        "The benchmark perform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tefj9DFfqXaC"
      },
      "source": [
        "**Threshold tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfKZDKcg7DYG"
      },
      "source": [
        "It is more flexible to predict probabilities of a loan belonging to each class rather than predicting classes directly. Probabilities may be interpreted using different thresholds that allow us to trade-off concerns in the errors made by the model, such as the number of false negatives which outweighs the cost of false positives. We will use ROC Curves and Precision-Recall curves as diagnostic tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpymr3fvqXaE"
      },
      "source": [
        "predict_evaluate_cm(randomf_optim, X_val, y_val, threshold=.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyh1mcacj5cu"
      },
      "source": [
        "As shown above if we set the threshold too high, we end up with a very bad prediction of Charge Offs. Although we predict all Fully Paid loans right, our investor will have to deal with a very high number of loans failures.\n",
        "\n",
        "Below, by choosing a lower threshold the investor will miss some business opportunities, but will secure his finance by correctly identifying those loans with high risk of failures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBp7Pds7DYI",
        "scrolled": true
      },
      "source": [
        "predict_evaluate_cm(randomf_optim, X_val, y_val, threshold=.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZWH8Fmj5c0"
      },
      "source": [
        "By choosing a **threshold of 0.3**, we have ca. 12% of false positives. These are loans which have charged off but our model failed to identify them. We will assume that such a threshold is an acceptable compromise for our investment strategy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "771c2eYij5c2"
      },
      "source": [
        "threshold = .3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2KKv4RCqXaM",
        "scrolled": true
      },
      "source": [
        "predict_evaluate_cm(randomf_optim, X_val, y_val, threshold=threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7CE6RJlj5c8"
      },
      "source": [
        "### ROC benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NppvX3tNj5c-"
      },
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(15,10))\n",
        "models = [randomf_optim, dt_model, log_reg, ab_model, xgb_model, mlp_model, qda_model, nb_model, svm_model, knn_model]\n",
        "for model in models:\n",
        "    model_name = model.__class__.__name__\n",
        "    if  model_name == 'RandomForestClassifier' or \\\n",
        "        model_name == 'XGBClassifier' or \\\n",
        "        model_name == 'AdaBoostClassifier' or \\\n",
        "        model_name == 'DecisionTreeClassifier':\n",
        "        Xval = X_val\n",
        "        yval = y_val\n",
        "    elif model_name == 'MultinomialNB':\n",
        "        Xval = X_val_desc_features\n",
        "        yval = y_val_desc\n",
        "    else:\n",
        "        Xval = X_val_scaled\n",
        "        yval = y_val\n",
        "    y_pred = df_y_preds[model_name]\n",
        "    y_pred_proba = df_y_pred_probas[model_name]\n",
        "    [fpr, tpr, thr] = roc_curve(yval, y_pred_proba)\n",
        "    \n",
        "    ax.plot(fpr, tpr, label='{0} - auc = {1:.2%}'.format(model_name,auc(fpr, tpr)))\n",
        "    if model_name == 'RandomForestClassifier':\n",
        "        idx = np.min(np.where(tpr > threshold))\n",
        "        ax.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', label='', color='b')\n",
        "        ax.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', label='', color='b')\n",
        "ax.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
        "plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
        "plt.title('Receiver operating characteristic (ROC) curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TJ_BWtHj5dE"
      },
      "source": [
        "Combining all predictions and creating a heatplot of their correlations gives us the following view. Supoprt Vector Machine is excluded from this plot since it computes distances instead of probabilities, which can only be estimated at the cost of extensive computation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zQn5_os7DYO"
      },
      "source": [
        "df_y_val_prob = df_y_pred_probas[df_y_pred_probas.columns.difference(['SVC'])]\n",
        "sns.heatmap(df_y_val_prob.corr())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL6N2qKRj5dG"
      },
      "source": [
        "It is pretty clear from these visualizations that the predictions of Naive Bayes classifier, Multilayer Perceptron, QDA, Logistic Regression are very opposite from predictions of other models. The tree-based classifiers are producing rvery similar results to each other, showing their similar internal structure. The predictions of KNN classifier are also very less correlated with the predictions of other classifiers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPuYfmZEj5dG"
      },
      "source": [
        "Below we compute the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores using the average probabilities from our classifiers (excluding support vector machine)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uacRsslj5dI"
      },
      "source": [
        "y_val_pred_avg = adjust_proba(df_y_val_prob.mean(axis=1), threshold)\n",
        "avg_score = roc_auc_score(y_val, y_val_pred_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jsloMTXj5dK"
      },
      "source": [
        "y_val_pred_rf = df_y_preds['RandomForestClassifier']\n",
        "rf_score = roc_auc_score(y_val, y_val_pred_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIxdN8Clj5dO"
      },
      "source": [
        "print(\"With probability threshold {} on validation set\", threshold)\n",
        "print(\"  ROC AUC score obtained by averaging probabilities of several classifiers is {0:.2%}\".format(avg_score))\n",
        "print(\"  ROC AUC score obtained by the Random Forest Classifier is {0:.2%})\".format(rf_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iolhz385j5dS"
      },
      "source": [
        "This result suggests that an Ensemble technique, e.g. Stacking would most probably not bring considerable improvment in the reduction of false negatives (loan predicted fully paid, but in reality charge off). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReikqzJUqXaY"
      },
      "source": [
        "# Investment Strategy\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Accuracy, ROC, MSE, are terms familiar to a data scientist but, probably not terms an investor would easilly understand. One of the most common metrics to evaluate an investment is Return On Investment (ROI) which can be defined as (FinalValue - InitialValue)/InitialValue. For example, we invest \\\\$1000 and after a year, our investment is worth \\$1200, then our annual ROI is 20%.\n",
        "\n",
        "We will test our strategy using the 2015 LC loan stats data. 15.68% of the loans in the dataset are classified as False (failed).<br><br>\n",
        "The goal is to select loans such that the failure rate is less than 15.68%. At this point we are assumung that a selection of loans from the same grade (same interest rate) with a failure rate lower than 15.68% will yield a better ROI for the investor. This needs to be verified and involves looking at the costs of failed loans.<br><br>\n",
        "The Random Forest model that was developed in the previous section is used here since it clearly outperformed many different models we have been trying based on the ROC_AUC metric. <br><br>\n",
        "A failure rate of 15.68% or more which sounds excellent, but is indeed no better than a random loan selection.<br><br>\n",
        "Our Business Objective is to minimize the percentage of Loans selected which are False. This is referred to as the \"False Discovery Rate\" (FDR). It is equal to 1 - PPV (Positive Prediction Value) [13]. We may reject many loans which are good loans; however, we do not want to select loans that are bad ones.<br><br>\n",
        "\n",
        "As we saw in the confusion matrix in the previous sections, let's think about what happens when we reduce the threshold from, 50% to 30%. In this case, some loans which would have normally be classified as Fully Paid will be classified as Charge Off. However, the loans that are selected have a higher probability of truly being Charge Off (actual value). Similarly the probability of a selected loan failing will drop. Hence decreasing the threshold increases the quality of the selection with the trade-off that we are throwing away (not selecting) loans which would normally be classified as Good. \n",
        "\n",
        "In the next sections we will evaluate this strategy in loans of the same subgrade, as assigned by the LendingClub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkAtwZ3Mj5dS"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYczp834ynVq"
      },
      "source": [
        "Now it is time to evaluate our model on the left-out data set, which has not been used neither for training nor for cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqVRc1IBj5dU"
      },
      "source": [
        "y_test_pred = randomf_optim.predict_proba(X_test)[:, 1]\n",
        "y_test_pred = adjust_proba(y_test_pred, threshold)\n",
        "test_score = accuracy_score(y_test, y_test_pred)\n",
        "print(\"With probability threshold {0}, the Classification Accuracy Score on Test is {1:.2%}\".format(threshold, test_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZzKWw7Bj5dW"
      },
      "source": [
        "These are compeling results showing that the model separates fully paid from charge-off loans with high accuracy on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og9SSBhtj5de"
      },
      "source": [
        "print(\"Without our model, the accuracy on Test would have been {0:.2%}\".format(1-len(y_test[y_test==1])/len(y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnAg3tgTj5dm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FCmChoJj5dm"
      },
      "source": [
        "The function below fits a predictive model with a subset of data depending on the invesment strategy (sub grades and term), and it displays the accuracy of our predictions, as well as an estimation of the Return Of Investment campared to the default strategy offered by LendingClub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOtw2qAkqXa0"
      },
      "source": [
        "def simulate_strategy(df, X_test_df, X_test, sub_grades=['A1'], terms=[36], threshold=.3, verbose=True):\n",
        "    # indexes of test loans in the original data frame containing all loans\n",
        "    test_idx = X_test_df.index.values\n",
        "    # test loans from the original data frame containing all loans\n",
        "    df_test = df.loc[test_idx]\n",
        "    # subset of candidate test loans, filtered by specific sub grades and terms\n",
        "    df_test_candidates = df_test[(df_test.sub_grade.isin(sub_grades)) & (df_test.term.isin(terms))]\n",
        "    if df_test_candidates.shape[0] == 0:\n",
        "        return 0, 0\n",
        "    # indexes of candidate test loans\n",
        "    df_test_candidates_idx = df_test_candidates.index.values\n",
        "    # candidate test loans in the design matrix\n",
        "    X_test_candidates = X_test[df_test.index.isin(df_test_candidates_idx)]\n",
        "    y_test_candidates = y_test[df_test.index.isin(df_test_candidates_idx)]\n",
        "    # prediction of loan status for candidate test loans\n",
        "    y_test_candidates_pred = randomf_optim.predict_proba(X_test_candidates)[:, 1]\n",
        "    y_test_candidates_pred = adjust_proba(y_test_candidates_pred, threshold)\n",
        "    test_score = accuracy_score(y_test_candidates, y_test_candidates_pred)\n",
        "    df_test_candidates = df_test_candidates.assign(loan_status_predicted = y_test_candidates_pred)\n",
        "    # calculate the ROI for each candidate test loan\n",
        "    df_test_candidates['roi'] = df_test_candidates.apply(calc_roi, axis=1)\n",
        "    # calculate average annual ROI for the Lender's CLub 'Automatic' selection\n",
        "    roi_lc = df_test_candidates['roi'].mean()\n",
        "    # calculate average annual ROI for loans which our model predicts they will be fully paid (prediction is 0)\n",
        "    roi_randomf = df_test_candidates[df_test_candidates.loan_status_predicted==0]['roi'].mean()\n",
        "    # display results\n",
        "    if verbose:\n",
        "        print('Investment on sub grades {} and terms {}:'.format(sub_grades, terms))\n",
        "        print('')\n",
        "        print(\"  With probability threshold {0}, the selected loans were classified with accuracy {1:.2%}\".format(threshold, test_score))\n",
        "        print(\"  Average annual ROI for Lending Club is {}%\".format(round(roi_lc,2)))\n",
        "        print(\"  Average annual ROI for our model is {}%\".format(round(roi_randomf,2)))\n",
        "    return roi_lc, roi_randomf, df_test_candidates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y-Wm0LOj5do"
      },
      "source": [
        "#\n",
        "# Calculate ROI. Monthly data and much better documentation from Lender's Club is needed\n",
        "# to precisely calculate ROI. Still, this is not bad!\n",
        "#\n",
        "def calc_roi(row):\n",
        "# We assume here that  charge-offs are made in the last month\n",
        "    def insuf_payment(factor, i):\n",
        "\n",
        "        loss = int_rcvd + pmt_rcvd + net_recoveries +late_fees - owed\n",
        "        factor *= (1 + loss/principal)\n",
        "        factor = factor**(1/term)\n",
        "        return factor**12 - 1\n",
        "    \n",
        "    def early_payoff(factor, i):\n",
        "        return (factor**(1/(i+1)))**12 - 1 \n",
        "        \n",
        "    term = row.term\n",
        "    int_rcvd = row.total_rec_int\n",
        "    pmt_rcvd = row.total_pymnt\n",
        "    principal = row.funded_amnt\n",
        "    yr_int_rate = row.int_rate\n",
        "    mo_int_rate = (1+yr_int_rate)**(1/12) - 1\n",
        "    mo_pay = row.installment\n",
        "    \n",
        "    recoveries = row.recoveries\n",
        "    late_fees = row.total_rec_late_fee\n",
        "    fee = row.collection_recovery_fee\n",
        "    \n",
        "    net_recoveries = recoveries - fee\n",
        "    \n",
        "    owed = principal\n",
        "\n",
        "# Calculate monthly gain (could be less than 1) while there are months to go and interest\n",
        "# Could be extra interest because of hardship plans\n",
        "    factor = 1\n",
        "\n",
        "#---Beginning of For------------------------\n",
        "    for i in range(0, int(term)):\n",
        "\n",
        "        if (pmt_rcvd + .50 < mo_pay):\n",
        "             return insuf_payment(factor, i)\n",
        "        this_interest = owed*mo_int_rate     \n",
        "        \n",
        "# If not enough interest was paid to cover this month, consider loan paid off early\n",
        "# and count monthly gain as (1+mo_int_rate), just like NAR\n",
        "\n",
        "        factor *= 1 + mo_int_rate\n",
        "\n",
        "        if (this_interest > int_rcvd):\n",
        "            return early_payoff(factor, i)\n",
        "            \n",
        "        int_rcvd -= this_interest\n",
        "        if owed < mo_pay:\n",
        "            return early_payoff(factor, i)\n",
        "        pmt_rcvd -= mo_pay\n",
        "        owed -= mo_pay - this_interest\n",
        "#---End of For-------------------------------\n",
        "    return early_payoff(factor, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSZJdeufj5dq"
      },
      "source": [
        "We can simulate an improved return of investment on specific loans types below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5huaHvB3mqHI"
      },
      "source": [
        "roi_lc, roi_df, df_test_candidates = simulate_strategy(df_loan_accepted_census_cleaned, \n",
        "                                                               X_test_df, X_test, \n",
        "                                                               sub_grades=['D5'], \n",
        "                                                               terms=[36], \n",
        "                                                               threshold=.3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9zt_PELj5du"
      },
      "source": [
        "We can simulate the model on all sorts of subgrades/terms combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40b3S_ZHj5dw"
      },
      "source": [
        "sub_grades = df_loan_accepted_census_cleaned.sub_grade.unique()\n",
        "terms = df_loan_accepted_census_cleaned.term.unique()\n",
        "df_roi = pd.DataFrame(columns=['sub_grad', 'term', 'lc_roi', 'model_roi'])\n",
        "i = 0\n",
        "for sub_grade in sub_grades:\n",
        "    for term in terms:\n",
        "        roi_lc, roi_model = simulate_roi(df_loan_accepted_census_cleaned, \n",
        "                                           X_test_df, X_test, \n",
        "                                           sub_grades=[sub_grade], \n",
        "                                           terms=[term], \n",
        "                                           threshold=threshold,\n",
        "                                           verbose=False)\n",
        "        df_roi.loc[i]=[sub_grade, term, roi_lc, roi_model]        \n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz01mVXnj5dy"
      },
      "source": [
        "df_roi.lc_roi.mean(), df_roi.model_roi.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgM9m-yP-SZk"
      },
      "source": [
        "# Plots to check our model on fairness and discrimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faHStEsw-SZk"
      },
      "source": [
        "**The output of our model is the dataframe \"df_test_candidates\", selected loans by our model. We will use this information to our model for fairness and discrimination**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5RxgwaP-SZm"
      },
      "source": [
        "**Plot 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6RQfVw8-SZm"
      },
      "source": [
        "We investigate the correlation between census features and loan features.\n",
        "\n",
        "Looking at bottom left part of the heatmap below, we can say that the correlation between selected loan's features and census features is strongly negatively correlated. \n",
        "\n",
        "For instance 1, Being a female, they have very low cur balance and very low credit limit.\n",
        "\n",
        "For instance 2, A white race person can has a low value for \"mo_sin_old_il_acct\" and \"total_bc_limit\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dtLE7Kb-SZm"
      },
      "source": [
        "cols1 = ['loan_status','female_pct','male_pct','White_pct','Black_pct','Native_pct','Asian_pct','Hispanic_pct','poverty_level_below_pct','employment_2016_rate']\n",
        "cols2 = ['int_rate','term','dti','bc_open_to_buy','funded_amnt_inv','tot_hi_cred_lim','tot_cur_bal','annual_inc','total_bc_limit','bc_util','total_rev_hi_lim','mo_sin_old_rev_tl_op','total_bal_ex_mort','acc_open_past_24mths','mo_sin_old_il_acct']\n",
        "corr = df_test_candidates[cols1 + cols2].corr()\n",
        "mask = np.zeros_like(corr, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "ax.set_title('Correlation between loan and census information');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLUdN6Ve-SZq"
      },
      "source": [
        "**Plot 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sttzz9E0-SZq"
      },
      "source": [
        "We investigate fairness by displaying a stripplot with the distribution of census data.\n",
        "\n",
        "Overall it seems the likelihood of failing to fully payback loans is slightly affected by the race and gender. \n",
        "Being a black, asian, poverty level affects the probablity of payback loans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTGP-lAD-SZq"
      },
      "source": [
        "df = pd.melt(df_test_candidates[['loan_status','female_pct','male_pct','White_pct','Black_pct','Native_pct','Asian_pct','Hispanic_pct','poverty_level_below_pct','employment_2016_rate']], \"loan_status\", var_name=\"measurement\")\n",
        "df.head()\n",
        "\n",
        "f, ax = plt.subplots(figsize=(10,7))\n",
        "sns.despine(bottom=True, left=True)\n",
        "sns.stripplot(x=\"value\", y=\"measurement\", hue=\"loan_status\",\n",
        "              data=df, dodge=True, jitter=True, alpha=.25,zorder=1)\n",
        "sns.pointplot(x=\"value\", y=\"measurement\", hue=\"loan_status\",\n",
        "              data=df, dodge=.532, join=False, palette=\"dark\",\n",
        "              markers=\"d\", scale=.75, ci=None)\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles[3:], labels[3:], title=\"loan status\",\n",
        "          handletextpad=0, columnspacing=1,\n",
        "          loc=\"lower right\", ncol=3, frameon=True)\n",
        "ax.set_title('Consensus data vs loan');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43oIE29c-xCI"
      },
      "source": [
        "## Algorithmic Fairness\n",
        "\n",
        "A high level goal of this project is addressing the ethic implications of our model along the lines of fairness and discrimination.\n",
        "\n",
        "It is a known fact that algorithms can facilitate illegal discrimination. \n",
        "\n",
        "An investor wants to invest in loans with high return of investment and low risk. A modern idea is to use an algorithm to decide, based on the sliver of known information about the outcome of past loans, which future loan requests give the largest chance of the borrower fully paying it back, while achieving the best trade-off with high returns (high interest rate). Our model is such an algorithm.\n",
        "\n",
        "There’s one problem: our model is trained on historical data, and poor uneducated people, often racial minorities or people with less working experience have a historical trend of being more likely to succumb to loan charge-off than the general population. So if our model is trying to maximize return of investment, it may also be targeting white people, people on specific zip codes, people with work experience, de facto denying opportunities for fair loans to the remaining population.\n",
        "\n",
        "Such behavior would be illegal [15].\n",
        "\n",
        "There could be two points of failure here: \n",
        "\n",
        "- we could have unwittingly encode biases into the algorithm based on a biased exploration of the data\n",
        "- the data itself could encode biases due to human decisions made to create it\n",
        "\n",
        "Although there is no definition which is widely agreed as a good definition of fairness, we will use **statistical parity** to test hypothesis of fairness on our model.\n",
        "\n",
        "Given a sub grade and a term, assuming that the population of borrowers who applied for a loan of that term and were assigned the sub grade is called $P$, and there is a known subset $F$ of female borrowers within that population. We assume that there is some distribution $D$ over $P$ which represents the probability that any of those borrowers will be picked by our model for evaluation. Our model is a classifier $m: X \\rightarrow \\{0, 1\\}$ that gives labels borrowers. if $m=1$ then the person will Charge Off on his loan, if $m=0$, he will fully pay his loan. The bias or **statistical imparity** of $m$ on $F$ with respect to $X, D$ is the difference between the probability that a random female borrower is labeled 1 and the probability that a random male borrower is labeled 1. If that difference is small, then we can say that our model is having statistical parity [17].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL9VR_D3-198"
      },
      "source": [
        "The function below measures the statistical parity of our model on protected population $F$. This metric describes how fair our model is with respect to the protected subset population. The input of the function is an array of binary values (1 if the sample is a loan requested by a female, 0 else) and a second array of binary values (1 if the model predicted that the loan will Charge Off, 0 else).  The code is adapted from [16]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZuWbh1A-x0M"
      },
      "source": [
        "def statistical_parity(protected_status, loan_status_predicted):\n",
        "\n",
        "    if len(protected_status) != len(loan_status_predicted):\n",
        "        raise ValueError('Input arrays do not have same number of entries')\n",
        "\n",
        "    indices_pos_class, = np.where(protected_status == 1)\n",
        "    indices_neg_class, = np.where(protected_status == 0)\n",
        "\n",
        "    outcomes_pos = loan_status_predicted[indices_pos_class]\n",
        "    outcomes_neg = loan_status_predicted[indices_neg_class]\n",
        "\n",
        "    if len(outcomes_pos) == 0:\n",
        "        return None\n",
        "    if len(outcomes_neg) == 0:\n",
        "        return None\n",
        "    value_discrim = np.abs(len(np.where(outcomes_pos == 1)) /\n",
        "                           len(outcomes_pos) -\n",
        "                           len(np.where(outcomes_neg == 1)) /\n",
        "                           len(outcomes_neg))\n",
        "\n",
        "    return value_discrim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vgRFiBO-7sM"
      },
      "source": [
        "The function below test statistical parity on a group of features of the borrowers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q36z4MC9-4ZC"
      },
      "source": [
        "def model_fairness_check(df, protected_groups, fairness_threshold):\n",
        "    for group in protected_groups:\n",
        "        protected_status = df_test_candidates[group].apply(lambda x: 1 if x>0.5 else 0)\n",
        "        loan_status_predicted = df_test_candidates['loan_status_predicted'].values\n",
        "        stats_parity = statistical_parity(protected_status, loan_status_predicted)\n",
        "        if stats_parity == None:\n",
        "            print(\"Not enough data available for {}\".format(group))\n",
        "        else:\n",
        "            if abs(stats_parity) < fairness_threshold:\n",
        "                print(\"The model is FAIR on {} with statistical parity {}\".format(group, round(stats_parity,5)))\n",
        "            else:\n",
        "                print(\"The model is NOT FAIR on {} with statistical parity {}\".format(group, round(stats_parity,5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-loqc5FA_DKQ"
      },
      "source": [
        "Now we can test the fairness of our model with regard to specifics borrowers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n16fh9Pi-9l6"
      },
      "source": [
        "model_fairness_check(df_test_candidates, ['female_pct','Black_pct', 'Native_pct', 'Asian_pct','Hispanic_pct', 'household_family_pct', 'poverty_level_below_pct','Graduate_Degree_pct'], fairness_threshold=.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKeubJ6I_Jzs"
      },
      "source": [
        "**Disparate impact** in United States labor law refers to \"practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral.\"[18]. It measures the difference that the majority and protected classes get a particular outcome. We use the function below adapted from [16] to check on disparate impact based on the legal definition of threshold of 80%. This is determined with respect to a protected class.\n",
        "\n",
        "The input of the function is an array of binary values (1 if the sample is a loan requested by a female, 0 else) and a second array of binary values (1 if the model predicted that the loan will Charge Off, 0 else). If $\\dfrac{P(male | chargeoff )}{P(female | chargeoff )} \\leq 80%$ then the definition of disparate impact is satisfied. The output is True is the model demonstrates discrimination, False else. The degree of discrimation is also provided between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1rqOJiH_KcO"
      },
      "source": [
        "def disparate_impact(protected_status, loan_status_predicted):\n",
        "    if len(protected_status) != len(loan_status_predicted):\n",
        "        raise ValueError('Input arrays do not have same number of entries')\n",
        "\n",
        "    # \"positive class\" are those where predictions = Charge Off\n",
        "    # \"majority class\" are those where protected class status = 1\n",
        "    \n",
        "    indices_pos_class, = np.where(protected_status == 1)\n",
        "    \n",
        "    outcomes_pos = loan_status_predicted[indices_pos_class]\n",
        "\n",
        "    if len(np.where(outcomes_pos == 1)) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    value_discrim = len(np.where(outcomes_pos == 0)) / len(\n",
        "        np.where(outcomes_pos == 1))\n",
        "\n",
        "    if value_discrim <= 0.8:\n",
        "        is_discriminatory = True\n",
        "    else:\n",
        "        is_discriminatory = False\n",
        "\n",
        "    return is_discriminatory, 1 - value_discrim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I99yQEH6_ShU"
      },
      "source": [
        "The function below disparate impact on a group of features of the borrowers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPAMG-Rs_NMa"
      },
      "source": [
        "def model_disparate_impact_check(df, protected_groups):\n",
        "    for group in protected_groups:\n",
        "        protected_status = df_test_candidates[group].apply(lambda x: 1 if x>0.5 else 0)\n",
        "        loan_status_predicted = df_test_candidates['loan_status_predicted'].values\n",
        "        discriminate, level_discrimination = disparate_impact(protected_status, loan_status_predicted)\n",
        "        if discriminate == None:\n",
        "            print(\"Not enough data available for {}\".format(group))\n",
        "        else:\n",
        "            if discriminate:\n",
        "                print(\"The model is DISCRIMINATING on {} with level {}\".format(group, round(level_discrimination,5)))\n",
        "            else:\n",
        "                print(\"The model is NOT DISCRIMINATING on {}\".format(group))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD2okqBI_X10"
      },
      "source": [
        "Now we can test the disparate impact of our model with regard to specific borrowers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgC4qpxq_UlM"
      },
      "source": [
        "model_disparate_impact_check(df_test_candidates, ['female_pct','Black_pct', 'Native_pct', 'Asian_pct','Hispanic_pct', 'household_family_pct', 'poverty_level_below_pct','Graduate_Degree_pct'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9CO3uqrmqH8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}